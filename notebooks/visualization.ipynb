{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b381b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "SUPABASE_URL = \"SUPABASE_URL\"\n",
    "SUPABASE_KEY = \"SERVICE_ROLE_API_KEY\"\n",
    "\n",
    "endpoint = f\"{SUPABASE_URL}/rest/v1/news?select=*\"\n",
    "\n",
    "headers = {\n",
    "    \"apikey\": SUPABASE_KEY,\n",
    "    \"Authorization\": f\"Bearer {SUPABASE_KEY}\"\n",
    "}\n",
    "\n",
    "resp = requests.get(endpoint, headers=headers)\n",
    "resp.raise_for_status()\n",
    "\n",
    "pdf = pd.DataFrame(resp.json())\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ca25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from pyspark.sql.functions import col, trim\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0bbfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d4088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total data sebelum pembersihan:\", df.count())\n",
    "\n",
    "df_clean = (\n",
    "    df\n",
    "    .dropna(subset=[\"title\"])\n",
    "    .filter(trim(col(\"title\")) != \"\")\n",
    "    .dropDuplicates([\"title\"])\n",
    "    .select(\"title\")\n",
    ")\n",
    "\n",
    "print(\"Total data setelah pembersihan:\", df_clean.count())\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df_clean.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aef623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "\n",
    "    text = re.sub(\n",
    "        \"[\" \n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002700-\\U000027BF\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        \"\",\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # hapus SEMUA selain huruf\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "    # rapikan spasi\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc45da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ea583",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_dict = {\n",
    "    \"gk\": \"tidak\", \"ga\": \"tidak\", \"gak\": \"tidak\", \"nggak\": \"tidak\",\n",
    "    \"ngga\": \"tidak\", \"tdk\": \"tidak\", \"tak\": \"tidak\",\n",
    "    \"udh\": \"sudah\", \"udah\": \"sudah\", \"sdh\": \"sudah\",\n",
    "    \"dr\": \"dari\", \"krn\": \"karena\", \"pdhl\": \"padahal\",\n",
    "    \"yg\": \"yang\", \"dgn\": \"dengan\", \"dlm\": \"dalam\",\n",
    "    \"aja\": \"saja\", \"bgt\": \"banget\", \"jg\": \"juga\"\n",
    "}\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    return [normalization_dict.get(t, t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d84bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_factory = StopWordRemoverFactory()\n",
    "default_stopwords = set(stopword_factory.get_stop_words())\n",
    "\n",
    "custom_stopwords = {\n",
    "    \"nih\",\"sih\",\"dong\",\"deh\",\"kan\",\"lah\",\"nya\",\"kok\",\"loh\",\"pun\",\n",
    "    \"aja\",\"banget\",\"cuma\",\"doang\",\"gitu\",\n",
    "    \"iya\",\"ya\",\"oh\",\"eh\",\"hehe\",\"hmm\",\"wkwk\",\"haha\",\n",
    "    \"udah\",\"lagi\",\"masih\",\"dah\",\"telah\",\"baru\",\"akan\",\n",
    "    \"ingin\",\"harus\",\"boleh\",\"bisa\",\"sangat\",\"sekali\"\n",
    "}\n",
    "\n",
    "negation_words = {\"tidak\",\"tak\",\"gak\",\"nggak\",\"ngga\",\"bukan\",\"belum\"}\n",
    "stopwords = (default_stopwords | custom_stopwords) - negation_words\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in stopwords]\n",
    "\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "def stemming_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = tokenize_text(cleaned)\n",
    "    normalized = normalize_tokens(tokens)\n",
    "    no_stopwords = remove_stopwords(normalized)\n",
    "    stemmed = stemming_tokens(no_stopwords)\n",
    "    return stemmed\n",
    "\n",
    "pdf[\"title_clean\"] = pdf[\"title\"].apply(preprocess_text)\n",
    "pdf[[\"title\", \"title_clean\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1dade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc_path = \"/Volumes/workspace/drive/kamus-nrc/NRC-Emotion-Lexicon (1).csv\"\n",
    "nrc_df = pd.read_csv(nrc_path)\n",
    "\n",
    "print(\"Kolom NRC:\", nrc_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_dict = {}\n",
    "\n",
    "for _, row in nrc_df.iterrows():\n",
    "    score = {\n",
    "        \"positive\": int(row.get(\"Positive\", 0)),\n",
    "        \"negative\": int(row.get(\"Negative\", 0))\n",
    "    }\n",
    "\n",
    "    # Bahasa Indonesia\n",
    "    id_word = row.get(\"Indonesian (id)\")\n",
    "    if isinstance(id_word, str):\n",
    "        lexicon_dict[id_word.lower()] = score\n",
    "\n",
    "    # Bahasa Inggris (jaga-jaga)\n",
    "    en_word = row.get(\"English (en)\")\n",
    "    if isinstance(en_word, str):\n",
    "        lexicon_dict[en_word.lower()] = score\n",
    "\n",
    "print(\"Jumlah kata lexicon:\", len(lexicon_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29423dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(tokens, lexicon):\n",
    "    pos, neg = 0, 0\n",
    "\n",
    "    if not isinstance(tokens, list):\n",
    "        return pos, neg\n",
    "\n",
    "    for t in tokens:\n",
    "        if t in lexicon:\n",
    "            pos += lexicon[t][\"positive\"]\n",
    "            neg += lexicon[t][\"negative\"]\n",
    "\n",
    "    return pos, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_label(tokens, lexicon):\n",
    "    pos, neg = sentiment_score(tokens, lexicon)\n",
    "\n",
    "    if pos >= neg:\n",
    "        return \"Positif\"\n",
    "    else:\n",
    "        return \"Negatif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf[\"sentiment\"] = pdf[\"title_clean\"].apply(\n",
    "    lambda tokens: sentiment_label(tokens, lexicon_dict))\n",
    "display(pdf[[\"title\", \"title_clean\", \"sentiment\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb715e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = pdf[\"sentiment\"].value_counts()\n",
    "display(sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707286a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf[\"text\"] = pdf[\"title_clean\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "pdf[\"label\"] = pdf[\"sentiment\"].map({\n",
    "    \"Negatif\": 0,\n",
    "    \"Positif\": 1\n",
    "})\n",
    "\n",
    "pdf[[\"text\", \"label\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pdf[\"text\"],\n",
    "    pdf[\"label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=pdf[\"label\"]\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(X_train))\n",
    "print(\"Test :\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84bbe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd706abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_model = LinearSVC(\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=5000\n",
    ")\n",
    "\n",
    "svm_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        target_names=[\"Negatif\", \"Positif\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine all processed text into a single string\n",
    "pdf = df.select(\"title\").toPandas()\n",
    "\n",
    "all_words = ' '.join(\n",
    "    [' '.join(t) if isinstance(t, list) else str(t) for t in pdf['title']]\n",
    ")\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white'\n",
    ").generate(all_words)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Processed Text')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
